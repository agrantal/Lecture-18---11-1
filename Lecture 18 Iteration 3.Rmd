---
title: "Lecture 18 Iteration 3"
output: html_document
---

#from last class

create dataframe that has info about what amazon page the info comes from along with other info from each page 
```{r}
tibble(
  page = 1:5,
  url = str_c(url_base, page)
) %>%
  mutate(reviews = map(url, read_page_review)) %>%
  unnest()
```

keeps track of cell range and move names that go along with it 
```{r}
lotr_cell_ranges = 
  tibble(
    movie = c("fellowship_ring", "two_towers", "return_king"),
    cells = c("B3:D6", "F3:H6", "J3:L6")
  )

lotr_tidy = 
  lotr_cell_ranges %>% 
  mutate(
    word_data = map(cells, ~readxl::read_excel("./data/LotR_Words.xlsx", range = .x)) #running from here up gives you movie, cell name, and word data
  ) %>% 
  unnest() %>% #running from here up gives you movie, cells, race, female, male 
  janitor::clean_names() %>% 
  gather(key = sex, value = words, female:male) %>%
  mutate(race = tolower(race)) %>% 
  select(movie, everything(), -cells) 
```





# Simulations

```{r}
library(tidyverse)
set.seed(1)
```

## Simple linear regression for one n

Simulate data from a simple linear regression, fit the regression model, and return estimates of regression coefficients
```{r}
sim_regression = function(n, beta0 = 2, beta1 = 3) {
  sim_data = tibble(
    x = rnorm(n, mean = 1, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, 0, 1) #rnorm = the error term 
  )
  ls_fit = lm(y ~ x, data = sim_data) #fit linear model to sim_data that was just created
  tibble(
    beta0_hat = coef(ls_fit)[1],
    beta1_hat = coef(ls_fit)[2]
  )
}
```

### run it a few times 

```{r}
sim_regression(n = 30, beta0 = 2, beta1 = 3)
#this defines a sample size of 30, a slope of 3 and an intercept of 2 and gives you a sense of variability in beta0 and beta1
```

### run it MANY MANY times --> Iterate! 

it's much easier to do repeated sampling using the exact same process (as is done in public health) using a computer... we can define a population and use a random process to generate data and then statiscally analyze it 

run regression 100 times to see effect of randomness on estimates
```{r}
output = vector("list", 100)

for (i in 1:100) {     #in general, don't use for loops 
  output[[i]] = sim_regression(30) #how to save output
}

sim_results = bind_rows(output)

sim_results = 
  rerun(100, sim_regression(30, 2, 3)) %>% 
  bind_rows()  #collapse outputs into single dataframe 
```

Structurally, rerun is a lot like map – the first argument defines the amount of iteration and the second argument is the function to use in each iteration step. As with map, we’ve replaced a for loop with a segment of code that makes our purpose much more transparent but both approaches give the same results.

###verify that formulas that we derived
```{r}
sim_results %>%
  summarise(mean_b0 = mean(beta0_hat),
            mean_b1 = mean(beta1_hat))
```

### plot and compute summaries for our simulation results.
```{r}
sim_results %>% 
  ggplot(aes(x = beta0_hat, y = beta1_hat)) + 
  geom_point()
#plot shows that mean and intercept are correlated

sim_results %>% 
  gather(key = parameter, value = estimate, beta0_hat:beta1_hat) %>% 
  group_by(parameter) %>% 
  summarize(emp_mean = mean(estimate),
            emp_var = var(estimate)) %>% 
  knitr::kable(digits = 3)
```

## Simple linear regressions for several n's (a few sample sizes)

A better way to iterate, which is necessary when sample size increases alot

```{r}
n_list = list("n_30"  = 30, #defining each sample size, so first dataframe will have a sample size of 30
              "n_60"  = 60, #the second dataframe will have a sample size of 60
              "n_120" = 120, 
              "n_240" = 240)
output = vector("list", length = 4)

for (i in 1:4) {
  output[[i]] = 
    rerun(100, sim_regression(n_list[[i]])) %>%  #can define rerun any number of times you want to e.g. 100 
    bind_rows
}

simulate_n_regressions = function(n_runs = 100, n, beta0 = 2, beta1 = 3) {  #sample size of 100 
  rerun(n_runs, sim_regression(n, beta0, beta1)) %>% #map the first line into this second line  
    bind_rows()
}

#make sure as you experiment with varying your arguments/parameters, the function still behaves in the way you want/defined:
simulate_n_regressions(100, 30, 2, 3) 
simulate_n_regressions(150, 30, 20, 3)

#what is this??????:
map(n_list, ~simulate_n_regressions(n_runs = 100, n = .x, beta0 = 2, beta1 = 3))
```

### Use a list column
```{r}
sim_results = 
  tibble(sample_size = c(30, 60, 120, 240)) %>% 
  mutate(estimate_dfs = map(.x = sample_size, ~simulate_n_regressions(n_run = 1000, n = .x))) %>% 
  unnest
#originally, we wrote simulate_n_regressions to run 100 times, but if we want to change the # of times the regression is run per sample size to e.g. 1000 we'd add "n_run = 1000" -- this isn't necessary but in general, it's helpful because to get a more accurate empirical variance 

# this will give you one dataframe with estimates (slope and intercept) for sample size 30, another dataframe for sample size of 60, etc, etc 
```

### to see how the variance in slope changes as sample size changes

this impt in determining how large a sample size has to be to get a normal distribution 

```{r}
sim_results %>% 
  mutate(
    sample_size = str_c("n = ", sample_size),
    sample_size = fct_inorder(sample_size)) %>% 
  ggplot(aes(x = sample_size, y = beta1_hat, fill = sample_size)) + 
  geom_violin()
#plot shows that as sample size increases, the variance in slope decreases
```

### to see bivariate distribution of variance in slope and intercept changes as sample size changes

```{r}
#we see same trend as in violin plot above 
sim_results %>% 
  mutate(
    sample_size = str_c("n = ", sample_size),
    sample_size = fct_inorder(sample_size)) %>% 
  ggplot(aes(x = beta0_hat, y = beta1_hat)) + 
  geom_point(alpha = .2) + 
  facet_grid(~sample_size)
```

### Find empirical mean and variance of these estimates

This is helpful to check derivations
```{r}
sim_results %>% 
  gather(key = parameter, value = estimate, beta0_hat:beta1_hat) %>% 
  group_by(parameter, sample_size) %>% 
  summarize(emp_mean = mean(estimate),
            emp_var = var(estimate)) %>% 
  knitr::kable(digits = 3)
```


## simulation: publication bias

```{r}
sim_regression = function(n_samp = 30, beta0 = 2, beta1 = 3) {
  
  sim_data = tibble(
    x = rnorm(n_samp),
    y = beta0 + beta1 * x + rnorm(n_samp, 0, sqrt(50)) #changed standard deviation from 1 to 50
  )
  ls_fit = lm(y ~ x, data = sim_data)
  broom::tidy(ls_fit)  #to cleanly format
}
```

instead of varying sample size, we'll vary slope value from 0 to 6
```{r}
sim_results = 
  tibble(beta1_true = 0:6) %>% 
  mutate(
    estimate_dfs = map(.x = beta1_true, ~simulate_n_regressions(n_runs = 10000, n = 30, beta1 = .x))
  )  
```

```{r}
sim_results = 
  sim_results %>% 
  unnest() %>%     #remember to unnest! 
  filter(term == "x") %>% 
  select(beta1_true, estimate, p.value) %>% 
  mutate(significant = as.numeric(p.value < 0.05))   #create var that identifies if estimate is statistically sig (less than or equal to 0.05)
```

### to investigate relationship between true slope and power 
```{r}
sim_results %>%
  group_by(beta1_true) %>%
  summarize(mean_est = mean(estimate), 
            power = mean(significance))
```

###To make plot of true slope to average effect size 

shows that if you only have access to articles that publish p >= 0.05, you'll overestimate the average effect size than if you have access to ALL articles regardless of p

this happens more when you have relatively low power or low effect size ??????
```{r}
results_summary %>%
  sim_results %>%
  group_by(beta1_true) %>%
  nest() %>%
  mutate(
      all = map_dbl(.x = data, ~.x %>% pull(estimate) %>% mean), 
      signif = map_dbl(.x = data, ~ .x %>% filter(significant == 1) %>% pull(estimate) %>% mean)
  ) %>%
  select(~data) %>%
.
.
.
.
.
sim_results %>% 
  ggplot(aes(x = estimate)) + geom_histogram() + 
  facet_grid(significant ~ beta1_true)
```




